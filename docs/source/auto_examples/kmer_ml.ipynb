{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# kmer_ml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n# coding: utf-8\n\n# Author: Ge Zhou\n# Email: ge.zhou@kaust.edu.sa\n\n\n\nimport os\nfrom scipy.sparse import csr_matrix\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport pickle\n#import matplotlib.pyplot as plt\nimport shap\n#import importlib\n#importlib.reload(shap)\n#shap.initjs()\nimport sys\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_validate\nfrom sklearn.metrics import confusion_matrix, classification_report, balanced_accuracy_score, roc_auc_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom datetime import datetime\nfrom math import sqrt\nimport warnings\ntestflag = False\n\nwarnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# K-mer dataset processing\n\n## Starting with reading chunk files(2000 in total)\n\n\n# Function to retrieve all file names in a specified directory \n# that start with the string 'chunk' followed by a numerical value.\ndef file_name_listdir_local(file_dir):\n    files_local = []  # Initialize an empty list to hold the file names.\n    for files in os.listdir(file_dir):  # Loop through all files in the specified directory.\n    #    print(files)  # (Optional) print the name of each file (line commented out).\n        # Check if the file name starts with 'chunk' and the rest of the name is a numerical value.\n        if files.startswith('chunk') and files[6:].isdigit():  \n            files_local.append(files)  # If condition met, add the file name to the list.\n    return files_local  # Return the list of file names meeting the specified criteria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "## Generating a Sparse Matrix and Establishing Cut-off Values (0.01, 0.05, 0.1...) for Filtering\n\n# Define a function named 'get_datamatrix' with parameters 'row_list', 'files_select', 'datapath', and 'cutoff'\ndef get_datamatrix(row_list,numb_files_select,datapath='./',cutoff=0.05):\n    # Call file_name_listdir_local function to obtain a list of files from the specified directory\n    files_local = file_name_listdir_local(datapath)\n\n    # Restrict the list of files to the first 'files_select' number of files\n        \n    if numb_files_select > 0 and numb_files_select < len(files_local):\n        files_local = files_local[0:numb_files_select]\n\n    # Initialize an empty dictionary to store the column vocabulary\n    vocabulary_col = {}\n\n    # Initialize an empty dictionary to store the row vocabulary\n    vocabulary_row = {}\n\n    # Populate the row vocabulary with the values from 'row_list'\n    for rl in row_list:\n        _ = vocabulary_row.setdefault(rl, len(vocabulary_row))\n\n    # Initialize counters and empty lists for further processing\n    num_removed = 0\n    num_preserved  = 0\n    row =[]\n    col = []\n    data = []\n    num_list = []  # This list is initialized but not used within the function\n\n    # Iterate through each file in the restricted list of files\n    for filename in files_local:\n        # Open and read the file in binary mode\n        with open(datapath + filename, 'rb') as f:\n            ob = f.readlines()\n            # Iterate through each line in the file\n            for lines in ob:\n                n = 0\n                linestr = lines.decode('utf-8').split()  # Decode the binary line to utf-8 and split it into words\n\n                # Count the occurrences of words starting with 'assembly' and ending with a digit\n                for s in linestr:\n                    if s.startswith('assembly'):\n                        n += int(s[-1])\n\n                # Check if the ratio of the count to 857.0 is greater than the cutoff value\n                if n/857.0 > cutoff:\n                    num_preserved += 1  # Increment the count of preserved lines\n                    indx_col = vocabulary_col.setdefault(linestr[0], len(vocabulary_col))  # Update the column vocabulary\n\n                    # Process each word in the line\n                    for s in linestr:\n                        if s.startswith('assembly'):\n                            sp = s.split(':')\n                            if sp[0][9:] in vocabulary_row:\n                                col.append(indx_col)  # Append the column index\n                                indx_row = vocabulary_row.get(sp[0][9:])  # Get the row index from the vocabulary\n                                row.append(indx_row)  # Append the row index\n                                data.append(int(sp[1]))  # Append the data value\n                else:\n                    num_removed += 1  # Increment the count of removed lines\n\n    # Calculate the percentage of removed lines\n    remove_percent = num_removed/(num_preserved + num_removed)\n\n    # Create a Compressed Sparse Row matrix from the data, row, and col lists\n    mtr = csr_matrix((data, (row, col)))\n\n    # Return the sparse matrix, column vocabulary, row vocabulary, and removal percentage as output\n    return mtr, vocabulary_col, vocabulary_row, remove_percent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "## Get and Filter Data: Acquire the specified labels from the dataset and filter out the two designated data types.\n## HH&HA HH&AA AND HA&AA\n\n\ndef get_datafilter(datalabel, filefold):\n    # Reading data from a CSV file and storing it in a DataFrame called df\n    if not filefold.endswith('/'):\n        filefold = filefold +'/'\n    df = pd.read_csv(filefold+'shortened_traits_scoary.csv')\n\n    # Extracting values from the input dictionary datalabel and storing them in variables HA, HH, and AA\n    HA = datalabel['HA']\n    HH = datalabel['HH']\n    AA = datalabel['AA']\n\n    # Creating a new column called \"data_type\" in df.\n    # This column is populated based on the values in the \"mixed_other\", \"human_other\", and \"animal_other\" columns of df.\n    df[\"data_type\"] = df.apply(lambda row: HA if row[\"mixed_other\"] == 1 \n                               else (HH if row[\"human_other\"] == 1 \n                                     else (AA if row[\"animal_other\"] == 1 else None)), axis=1)\n\n    # Filtering df to include only the rows where \"data_type\" is 0 or 1.\n    # Storing the result in a new DataFrame called filtered1_df.\n    filtered1_df = df[df['data_type'].isin([0, 1])]\n    filtered1_df= filtered1_df.set_index('Name')\n\n    # Returning the filtered DataFrame.\n    return filtered1_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def gridresearch_kfold(X,y,feature_names):\n    # Initialize empty lists to store the performance, method, report, roc_auc,\n    # best features, best feature indices, and best parameters\n    performance = []\n    method = []\n    report = []\n    roc_auc = []\n    bestfeature =[]\n    bestfeature_indices=[]\n    bestpara = []\n\n    # Run the process 5 times with different train-test splits\n    for i in range(5):\n        print(f\"Train-Test Split {i+1}\")\n        # Split the dataset into training and testing sets\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n        # Create a KFold object to split the training set into 5 folds\n        kfold = KFold(n_splits=5, shuffle=True)\n\n        # Loop through each fold\n        for fold, (train_index, test_index) in enumerate(kfold.split(X_train, y_train), 1):\n            # Split the training set into training and validation folds\n            X_train_fold, X_val_fold = X_train[train_index], X_train[test_index]\n            y_train_fold, y_val_fold = y_train[train_index], y_train[test_index]\n\n            # Define the parameters and the model for grid search\n            xgb_params = {'max_depth': [1, 3],'n_estimators': [20, 50],'subsample': [0.8, 1.0]}\n            xgb_model = xgb.XGBClassifier(objective='binary:logistic')\n\n            # Initialize GridSearchCV\n            gs_clf = GridSearchCV(estimator=xgb_model, param_grid=xgb_params, cv=3, n_jobs=-1, scoring='balanced_accuracy')\n            # Fit the model on the training fold\n            gs_clf.fit(X_train_fold, y_train_fold)\n\n            # Predict on the validation and test sets\n            y_pred = gs_clf.predict(X_val_fold)\n            pred_class_xgboost = gs_clf.predict(X_test)\n            pred_probs_xgboost = gs_clf.predict_proba(X_test)\n\n            # Get the best estimator and parameters from the grid search\n            best_estimator = gs_clf.best_estimator_\n            best_para = gs_clf.best_params_\n            bestpara.append(best_para)\n\n            # Get the feature importances from the best estimator\n            best_features = gs_clf.best_estimator_.feature_importances_\n\n            # Get the indices of the best features sorted in descending order\n            best_feature_indices = np.argsort(best_features)[::-1]\n            # Get the names of the best features\n            best_feature_names = [feature_names[i] for i in best_feature_indices]\n            bestfeature.append(best_feature_names)\n            bestfeature_indices.append(best_feature_indices)\n\n            print('best_featrure', best_feature_names[0:5])\n            # Get the classification report for the validation fold\n            report0 = classification_report(y_val_fold, y_pred, output_dict=True)\n\n            # Get the balanced accuracy score for the validation fold\n            score = balanced_accuracy_score(y_val_fold, y_pred)\n            performance.append(score)\n\n            report.append(report0)\n\n            print(\"Best hyperparameters for this train-test split:\")\n            print(gs_clf.best_params_)\n            print(score)\n            print(\"Confusion matrix for this fold:\")\n            print(confusion_matrix(y_val_fold, y_pred))\n\n    # If running on a Linux platform, save the results to a file\n\n    dataset = {\n        'performance':performance,\n        'method':method,\n        'report':report,\n        'roc_auc':roc_auc,\n        'bestfeature':bestfeature,\n        'bestfeature_indices':bestfeature_indices,\n        'bestpara':bestpara\n    }\n    return dataset\n    # current_datetime = datetime.now()\n    # datetime_string = current_datetime.strftime('%Y%m%d_%H%M')\n    #\n    # filename = f\"gridreasearch_{datetime_string}.pickle\"\n    # with open(filename,'wb') as f:\n    #     pickle.dump(dataset, f)\n    #\n    # # Return all the stored results\n    # return  performance, method, report, roc_auc, bestfeature,bestfeature_indices,bestpara"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# shap value \ndef get_shapvalue(X, y, sub_bestfeature_indices,sub_bestfeature_name, best_para,class_name):\n    X_subset = X[:,sub_bestfeature_indices]\n    X_train, X_test, y_train, y_test = train_test_split(X_subset, y, test_size=0.2)\n\n    # Create and train your best XGBoost model\n    best_model = xgb.XGBClassifier( objective='binary:logistic',\n                                      param_grid = best_para )\n\n    #gs_clf = GridSearchCV(estimator=estimator, param_grid=parameters, cv=3, n_jobs=-1, scoring='balanced_accuracy')\n    best_model.fit(X_train, y_train)\n\n    tmp =  []\n    datalabel = class_name\n    for k in datalabel:\n        if datalabel.get(k) <=1:\n            tmp.append(k)\n    key = tmp.copy()\n    for i in range(len(key)):\n        key[datalabel[tmp[i]]] = tmp[i]\n    exp_set = X_subset.copy()\n    \n    ## K sample\n    ksmp = 100\n    background_summary = shap.sample(exp_set, ksmp)\n\n    explainer = shap.KernelExplainer(best_model.predict, background_summary)\n\n    shap_values = explainer.shap_values(exp_set)   \n    \n    # data save\n    dataset = {\n        'shap_values':shap_values,\n        'exp_set':exp_set,\n        'sub_bestfeature_name':sub_bestfeature_name,\n        'sub_bestfeature_indices':sub_bestfeature_indices,\n        'key':key\n    }\n    #\n    # current_datetime = datetime.now()\n    # datetime_string = current_datetime.strftime('%Y%m%d_%H%M')\n    #\n    # filename = f\"shapeKsmp{ksmp}_{datetime_string}.pickle\"\n    # with open(filename,'wb') as f:\n    #     pickle.dump(dataset, f)\n\n    return dataset"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}